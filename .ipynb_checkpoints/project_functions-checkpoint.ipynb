{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import Required Packages\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from pandas.io.json import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define File Paths as Constants\n",
    "#TWITS_PATH = r\"../stocktwits_prediction/Data/twits\"\n",
    "#FINANCE_PATH = r\"../stocktwits_prediction/Data/csv\"\n",
    "TWITS_PATH = \"../Data/twits\"\n",
    "FINANCE_PATH = \"../Data/csv\"\n",
    "#Define Company Names and List of Tags as Constant\n",
    "COMPANY_NAMES = [\"apple\",\"boeing\", \"caterpillar\", \"cisco\", \"chevron\", \"dupont\", \"de\", \"nemours\", \"walt\", \n",
    "                 \"diseny\", \"facebook\", \"google\", \"vaneck\", \"goldman\", \"sachs\", \"ibm\", \"intel\", \"johnson\",\n",
    "                \"jpmorgan\", \"coca\", \"cola\", \"coca-cola\", \"mcdonalds\", \"3m\", \"merck\", \"microsoft\", \"nike\",\n",
    "                \"pfizer\", \"unitedhealth\", \"raytheon\", \"visa\", \"verizon\", \"walmart\", \n",
    "                 \"exxon\", \"mobil\"]\n",
    "\n",
    "TAGS = ['\\$FB','\\$GOOG','\\$AAPL', '\\$WB','\\$AXP','\\$BA','\\$CAT','\\$CSCO','\\$CVX','\\$DD','\\$DIS','\\$FB',\n",
    "        '\\$GE','\\$GS','\\$HD','\\$IBM','\\$INTC','\\$JNJ','\\$JPM','\\$KO','\\$MCD','\\$MMM','\\$MRK','\\$MSFT',\n",
    "        '\\$NKE','\\$PFE','\\$PG','\\$QQQ','\\$SPY','\\$TRV','\\$UNH','\\$UTX','\\$V','\\$VZ','\\$WMT','\\$XOM']\n",
    "\n",
    "LABELS = [\"Positive Return\", \"Negative Return\"]\n",
    "\n",
    "#Return ALl Files in Directory\n",
    "all_twitter = os.listdir(TWITS_PATH)\n",
    "all_finance = os.listdir(FINANCE_PATH)\n",
    "\n",
    "#Generate List of Tags\n",
    "all_tags = list()\n",
    "for file_name in all_twitter:\n",
    "    tag = '$' + file_name[:-4]\n",
    "    all_tags.append(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stock_twits_metrics\n",
    "###  - Inputs:\n",
    "        - raw_data: DataFrame (can use returned DataFrame from the text parser\n",
    "        - n (int): number of days (used for s_t calculation)\n",
    "        - file_name (str): name of file - ex. 'FB.txt'\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - Date: DateTime object representing the a specific date in the sample period\n",
    "        - bearish: total count of tweets tagged with bearish sentiment on that day\n",
    "        - bullish: total count of tweets tagged with bullish sentiment on that day\n",
    "        - none: total count of tweets tagged with neither bearish nor bullisn sentiment on that day\n",
    "        - message_volume: total number of messages on that day \n",
    "        - mv1_t:  percentage change in message volume with one day period\n",
    "        - mv10_t: percentage change in message volume with ten day period\n",
    "        - polarity: (bullish - bearish)/total messages on that day\n",
    "        - s_t: moving average of polarity over n days\n",
    "        - company: string containing the abbreviation of the company name\n",
    "###  - Additional Notes:\n",
    "        - function aggregates the data into values by date - does not include the text of the orignal tweets in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_twits_metrics(raw_data, n, file_name):\n",
    "    #Unwrap Sentiment Values to New Columns\n",
    "    raw_data['sentiment'] = raw_data['sentiment'].apply(lambda x:{} if pd.isna(x) else x)\n",
    "    sentiment_vals = pd.json_normalize(data=raw_data['sentiment'], meta=['class','name'])\n",
    "    sentiment_vals = sentiment_vals.drop(columns='name')\n",
    "    sentiment_vals = sentiment_vals.replace(np.nan, 'none', regex=True)\n",
    "\n",
    "    #Convert Date Strings to DateTime Objects\n",
    "    raw_data['created_at'] = pd.to_datetime(raw_data['created_at']).apply(lambda x: x.date())\n",
    "\n",
    "    #Create New Dataframe\n",
    "    temp = pd.concat([raw_data['body'], sentiment_vals, raw_data['created_at']],axis=1)\n",
    "    \n",
    "    #Aggregate Text From Tweets By Date\n",
    "    agg_text = temp.groupby('created_at')['body'].apply(lambda x: x.sum())\n",
    "    agg_text.to_frame()\n",
    "    \n",
    "    #Group by Unique Dates and Extract Value Counts\n",
    "    message_volume = temp['created_at'].value_counts().rename_axis('dates').reset_index(name='message_volume')\n",
    "    message_volume = message_volume.sort_values(by='dates').reset_index(drop=True)\n",
    "\n",
    "    #Create Pivot Table With Desired Statistics\n",
    "    temp['count'] = 1\n",
    "    table = temp.pivot_table(index=['created_at'], dropna=False,\n",
    "                             columns='class',values='count',\n",
    "                             fill_value=0,aggfunc=np.sum)\n",
    "    \n",
    "    #Merge Table with Message Volume\n",
    "    table = table.reset_index()\n",
    "    table = pd.concat([table,message_volume['message_volume']],axis=1)\n",
    "\n",
    "    #Calculate Metrics and Append to the DataFrame\n",
    "    table['mv1_t'] = table['message_volume'].diff(periods=1).div(table['message_volume'].shift(1))\n",
    "    table['mv10_t'] = table['message_volume'].div(table['message_volume'].rolling(10).mean())\n",
    "\n",
    "    table['polarity'] = table.apply(lambda date: (date.bullish - date.bearish)/date.message_volume, axis = 1)\n",
    "    table['s_t'] = table['polarity'].rolling(n).mean()\n",
    "    \n",
    "    #Append Company Name to DataFrame\n",
    "    name = file_name[:-4]\n",
    "    table['company'] = name\n",
    "    \n",
    "    #Append Text to DataFrame\n",
    "    result = pd.merge(table, agg_text, on='created_at')\n",
    "    \n",
    "    #Change Column Name to Date\n",
    "    result = result.rename(columns={\"created_at\":\"Date\"})\n",
    "    \n",
    "    result = result.dropna(how = 'any')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stock_twits_text_parser\n",
    "###  - Inputs:\n",
    "        - file (str): name of file - ex. 'FB.txt'\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - df: DataFrame with all rows containing multiple tagged companies dropped, all columns included - index reset to reflect new size\n",
    "###  - Additional Notes:\n",
    "        - None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_twits_text_parser(file):\n",
    "    #Read File\n",
    "    file_path = TWITS_PATH + '/' + file\n",
    "    raw_data = pd.read_json(file_path)\n",
    "\n",
    "    #Create New Column with List of Unique Tags\n",
    "    raw_data[\"instances\"] = (raw_data['body'].str.findall(f'(?i)({\"|\".join(TAGS)})')\n",
    "                                .apply(lambda x: list(set(map(str.upper,x))))\n",
    "                            )\n",
    "    #Drop All Rows With Multiple Tags and Reset Index\n",
    "    df = raw_data[raw_data['instances'].map(lambda x: len(x)) < 2]\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature_selector\n",
    "###  - Inputs:\n",
    "        - metrics (DataFrame): DataFrame containing the body text of the tweet eg. data['body'] along with other metrics\n",
    "        - labels (DataFrame) DataFrame containing the target labels\n",
    "        - target_label (str): name of the column containing the target label we want to use\n",
    "        - min_count(int) : minimum number of occurances so that the word is converted\n",
    "        - n (int): number of text features selected\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - df: DataFrame with top n text features and all numerical features, raw data and labels (20 + n columns total)\n",
    "###  - Additional Notes:\n",
    "        - text features will always be appended after the numerical metrics, raw data and labels - currently these are the first 20 columns: use df.iloc[:,20:] to extract only text features for analysis\n",
    "        - text features selected using chi squared statistics\n",
    "        - ***TODO: Currently the method selects features based on categorical outcomes (classification problem) this will need to be changed to allow for regression (explore a way to bin continuous target data) ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selector(metrics, labels, target_label, min_count, n):\n",
    "    #Merge DataFranes\n",
    "    result = pd.merge(metrics, labels, on='Date')\n",
    "    \n",
    "    #Remove Company Names By Adding Them To Stop Words\n",
    "    new_stop_words = text.ENGLISH_STOP_WORDS.union(COMPANY_NAMES)\n",
    "\n",
    "    #Vectorizer Parameters: Convert To Lowercase, Remove Stop Words With Company Names,Min Count 25, Laplace Smoothing\n",
    "    v = TfidfVectorizer(analyzer='word', lowercase=True,stop_words=new_stop_words, min_df = min_count, smooth_idf=True)\n",
    "    x = v.fit_transform(result['body'])\n",
    "\n",
    "    #Convert back to DataFrame\n",
    "    text_data = pd.DataFrame(x.toarray(),columns=v.get_feature_names())\n",
    "    \n",
    "    #Select n Best Features\n",
    "    X = text_data\n",
    "    Y = convert_to_binary_classification(result[target_label])\n",
    "    chi2_selector = SelectKBest(chi2, k=n)\n",
    "    X_best = chi2_selector.fit_transform(X,Y)\n",
    "    \n",
    "    data = pd.DataFrame(X_best)\n",
    "    df = pd.concat([result,data],axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# return_over_period_T\n",
    "###  - Inputs:\n",
    "        - file (str): name of file - ex. 'FB.csv'\n",
    "        - T (int): period based on which return is calculated (in days)\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - Date: DateTime object representing a specific date in the sample period\n",
    "        - OPEN, HIGH, LOW, VOLUME, CLOSE: prices each day with respect to the specific metric\n",
    "        - xxx_return: return for referenced metric over period T, where T is specified when the function is called\n",
    "###  - Additional Notes:\n",
    "        - function includes original data taken from the CSV file\n",
    "        - formula for return over period T is defined as: r(T) = (p(t+T) - p(t))/p(t), where t is current day and p(t) i the price at the current day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_over_period_T(file, T):\n",
    "    #Read File\n",
    "    file_path = FINANCE_PATH + '/' + file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    #Convert Date Strings to DateTime Objects\n",
    "    df['Date'] = pd.to_datetime(df['Date']).apply(lambda x: x.date())\n",
    "\n",
    "    #Calculate Metrics for OPEN, HIGH, LOW, VOLUME and CLOSE\n",
    "    df['open_return'] = -df['OPEN'].diff(periods = -T).div(df['OPEN'])\n",
    "    df['high_return'] = -df['HIGH'].diff(periods = -T).div(df['HIGH'])\n",
    "    df['low_return'] = -df['LOW'].diff(periods = -T).div(df['LOW'])\n",
    "    df['close_return'] = -df['CLOSE'].diff(periods = -T).div(df['CLOSE'])\n",
    "    \n",
    "    df = df.dropna(how='any')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert_to_binary_classification\n",
    "###  - Inputs:\n",
    "        - df (DataFrame): single column to be classified\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - Binary Labels: single column of binary labels representing positive and negative returns on the day\n",
    "###  - Additional Notes:\n",
    "        - Note that a negative return is defined as class 0 and a postive return or no return is defined as class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_binary_classification(df):\n",
    "    binary_classification = df.apply(lambda x: -1 if x < 0 else 1)\n",
    "    return binary_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split_data\n",
    "###  - Inputs:\n",
    "        - df (DataFrame): Processed DataFrame with features and labels to be split\n",
    "        - train_percentage (float): Percentage of data to be used as training data - assumes all other data is used as test\n",
    "        - features (str): toggles whether to use text only or text and numerical features (valid inputs: 'text', 'all')\n",
    "        - label (str): selects target label based on column name\n",
    "        - target_type (str): binary (bin) or continuous (cont)\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - data_dict (Dictionary): dictionary with 4 values corresponding to X_train, X_test, Y_train, Y_test (keys)\n",
    "###  - Additional Notes:\n",
    "        - None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, train_percentage, features, label, target_type):\n",
    "    #Determine Split Index Based on the Percentage Of Data to Be Used For Training\n",
    "    train_index = int(train_percentage*len(df.index)) + 1\n",
    "    \n",
    "    #Split Data\n",
    "    if(features == 'text'):\n",
    "        X_train = df.iloc[0:train_index,20:]\n",
    "        X_test = df.iloc[train_index:,20:]\n",
    "    elif(features == 'all'):\n",
    "        df1 = df.iloc[0:train_index:,np.r_[5:7,8]]\n",
    "        df2 = df.iloc[0:train_index,20:]\n",
    "        X_train = pd.concat([df1,df2],axis=1)\n",
    "        \n",
    "        df3 = df.iloc[train_index:,np.r_[5:7,8]]\n",
    "        df4 = df.iloc[train_index:,20:]\n",
    "        X_test = pd.concat([df3,df4],axis=1)\n",
    "    else:\n",
    "        raise ValueError('Invalid Parameter Input - features')\n",
    "        \n",
    "    if(target_type == 'bin' or target_type == 'cont'):\n",
    "        if(target_type == 'bin'):\n",
    "            Y = convert_to_binary_classification(df[label])\n",
    "        else:\n",
    "            Y = df[label]\n",
    "        Y_train = Y.iloc[0:train_index]\n",
    "        Y_test = Y.iloc[train_index:]\n",
    "    else:\n",
    "        raise ValueError('Invalid Parameter Input - target_type')\n",
    "    \n",
    "    #Return as a Dictionary\n",
    "    data_dict = dict()\n",
    "    \n",
    "    data_dict['X_train'] = X_train\n",
    "    data_dict['X_test'] = X_test\n",
    "    data_dict['Y_train'] = Y_train\n",
    "    data_dict['Y_test'] = Y_test\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create_metrics_table\n",
    "###  - Inputs:\n",
    "        - Y_train: training labels\n",
    "        - pred_train: predicted labels from training data\n",
    "        - Y_test: training labels\n",
    "        - pred_test: predicted labels from test data\n",
    "        - valid input types (series, numpy array, DF columns) - must be column vectors\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - table (DataFrame): table with REC, PREC, F1 amd ACC for training and test predictions\n",
    "###  - Additional Notes:\n",
    "        - None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_table(Y_train, pred_train, Y_test, pred_test):\n",
    "    #Calculate Metrics\n",
    "    REC_train = recall_score(Y_train,pred_train)\n",
    "    PREC_train = precision_score(Y_train, pred_train)\n",
    "    F1_train = f1_score(Y_train, pred_train)\n",
    "    ACC_train = accuracy_score(Y_train, pred_train)\n",
    "\n",
    "    REC_test = recall_score(Y_test,pred_test)\n",
    "    PREC_test = precision_score(Y_test,pred_test)\n",
    "    F1_test = f1_score(Y_test, pred_test)\n",
    "    ACC_test = accuracy_score(Y_test, pred_test)\n",
    "\n",
    "    #Create Table\n",
    "    rows = [[\"Training\", REC_train, PREC_train, F1_train, ACC_train], [\"Test\", REC_test, PREC_test, F1_test,ACC_test]]\n",
    "    table = pd.DataFrame(rows, columns = [\"Dataset\", \"Recall\", \"Precision\", \"F1 Score\", \"Accuracy\"])\n",
    "    table.set_index(\"Dataset\", inplace =True)\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot_confusion_matrix\n",
    "###  - Inputs:\n",
    "        - matrix (confusion matrix): confusion matrix generated from sklearn confusion_matrix\n",
    "        - data_set_name (str): name of dataset ex. 'training' or 'test' \n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - None - void type function\n",
    "        - Displays plot\n",
    "###  - Additional Notes:\n",
    "        - Wrapper for seaborn plot code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(matrix, data_set_name):\n",
    "    sns.heatmap(matrix, annot=True, fmt= \".3f\", xticklabels = LABELS, yticklabels = LABELS)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"Actual Label\")\n",
    "    plt.title(\"Confusion Matrix for \" + data_set_name + \" Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot_ROC_curve\n",
    "###  - Inputs:\n",
    "        - FPR: False Positive Rate - Generated from sklearn roc_curve\n",
    "        - TPR: True Positive Rate - Generated from sklearn roc_curve\n",
    "        - data_set_name (str): name of dataset ex. 'training' or 'test'\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - None - void type function\n",
    "        - Displays plot\n",
    "###  - Additional Notes:\n",
    "        - Wrapper for matplotlib plot code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC_curve(FPR, TPR, data_set_name):\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "    plt.plot(FPR, TPR)\n",
    "    plt.xlabel(\"False Positive\")\n",
    "    plt.ylabel(\"True Positive\")\n",
    "    plt.title(\"ROC Curve for \" + data_set_name + \" Data\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# action_signal_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Inputs:\n",
    "    - Y_train: training data\n",
    "    - Y_test: test data\n",
    "    - pred_train: prediction on training data\n",
    "    - pred_test: prediction on test data\n",
    "    - threshold: threshold to map action signals to\n",
    "\n",
    "## - Outputs\n",
    "    - action_signal_train: training data mapped to action signals\n",
    "    - action_signal_test: test data mapped to action signals\n",
    "    - action_signal_predict_train: prediction on training data mapped to action signals\n",
    "    - action_signal_predict_test: prediction on test data mapped to action signals\n",
    "\n",
    "## - Additional Notes\n",
    "    - For the action signals, a \"positive\" action signal is any value greater than the threshold, and is mapped to 1. A mapping of zero is if the value is less than the threshold, but greater than the negative of the threshold, and is a \"no action\" signal. A mapping of -1 is if the value is less than the negative of the threshold, and is a \"negative\" action signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_signal_mapping(Y_train, Y_test, pred_train, pred_test, threshold):\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    action_signal_train = np.copy(Y_train)\n",
    "    action_signal_train[action_signal_train < -1*threshold] = -1\n",
    "    action_signal_train[action_signal_train > threshold] = 1\n",
    "    action_signal_train[np.abs(action_signal_train) <= threshold] = 0\n",
    "\n",
    "    action_signal_test = np.copy(Y_test)\n",
    "    action_signal_test[action_signal_test < -1*threshold] = -1\n",
    "    action_signal_test[action_signal_test > threshold] = 1\n",
    "    action_signal_test[np.abs(action_signal_test) <= threshold] = 0\n",
    "\n",
    "    action_signal_predict_train = np.copy(pred_train)\n",
    "    action_signal_predict_train[action_signal_predict_train < -1*threshold] = -1\n",
    "    action_signal_predict_train[action_signal_predict_train > threshold] = 1\n",
    "    action_signal_predict_train[np.abs(action_signal_predict_train) <= threshold] = 0\n",
    "\n",
    "    action_signal_predict_test = np.copy(pred_test)\n",
    "    action_signal_predict_test[action_signal_predict_test < -1*threshold] = -1\n",
    "    action_signal_predict_test[action_signal_predict_test > threshold] = 1\n",
    "    action_signal_predict_test[np.abs(action_signal_predict_test) <= threshold] = 0\n",
    "\n",
    "    return action_signal_train, action_signal_test, action_signal_predict_train, action_signal_predict_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metric_mapping_action_signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Inputs\n",
    "     - action_signal_train: training data mapped to action signals\n",
    "     - action_signal_test: test data mapped to action signals\n",
    "     - action_signal_predict_train: prediction of training data mappped to action signals\n",
    "     - action_signal_predict_test: prediciton of test data mapped to action signals\n",
    "\n",
    "## - Outputs\n",
    "     - action_signal_train_metrics: training data mapped to action signals with \"no action\" signals removed\n",
    "     - action_signal_test_metrics: test data mapped to action signals with \"no action\" signals removed\n",
    "     - action_signal_predict_train_metrics: prediction of training data mappped to action signals with \"no action\" signals removed\n",
    "     - action_signal_predict_test_metrics: prediciton of test data mapped to action signals with \"no action\" signals removed\n",
    "\n",
    "## Additional notes:\n",
    "    - for the measurement of different metrics such as accuracy, precision, recall etc., we drop the all cases where a \"no action\" signal is predicted and the truth was \"positive\" or \"negative\" action. We also drop the reverse scenario, where the truth was \"no action\", and the prediction was \"positive\" or \"negative\" action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_mapping_action_signals(action_signal_train, action_signal_test, action_signal_predict_train, action_signal_predict_test):\n",
    "    action_signal_train_metrics = np.empty((action_signal_train.shape))\n",
    "    action_signal_test_metrics = np.empty((action_signal_test.shape))\n",
    "    action_signal_predict_train_metrics = np.empty((action_signal_predict_train.shape))\n",
    "    action_signal_predict_test_metrics = np.empty((action_signal_predict_test.shape))\n",
    "\n",
    "    train_len = action_signal_train.shape[0]\n",
    "    test_len = action_signal_test.shape[0]\n",
    "\n",
    "    for i in range(0, train_len):\n",
    "        if (np.abs(action_signal_train[i] + action_signal_predict_train[i]) == 1) or (action_signal_train[i] == 0 and action_signal_predict_train[i] == 0):\n",
    "            action_signal_train_metrics[i] = 0\n",
    "            action_signal_predict_train_metrics[i] = 0\n",
    "        else:\n",
    "            action_signal_train_metrics[i] = action_signal_train[i]\n",
    "            action_signal_predict_train_metrics[i] = action_signal_predict_train[i]\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(0, test_len):\n",
    "        if (np.abs(action_signal_test[j] + action_signal_predict_test[j]) == 1) or (action_signal_test[j] == 0 and action_signal_predict_test[j] == 0):\n",
    "            action_signal_test_metrics[j] = 0\n",
    "            action_signal_predict_test_metrics[j] = 0\n",
    "        else:\n",
    "            action_signal_test_metrics[j] = action_signal_test[j]\n",
    "            action_signal_predict_test_metrics[j] = action_signal_predict_test[j]\n",
    "            \n",
    "    \n",
    "        \n",
    "    action_signal_train_metrics =  action_signal_train_metrics[action_signal_train_metrics != 0]\n",
    "    action_signal_test_metrics =  action_signal_test_metrics[action_signal_test_metrics != 0]\n",
    "    action_signal_predict_train_metrics = action_signal_predict_train_metrics[action_signal_predict_train_metrics != 0]\n",
    "    action_signal_predict_test_metrics = action_signal_predict_test_metrics[action_signal_predict_test_metrics != 0]\n",
    "    \n",
    "\n",
    "    return action_signal_train_metrics, action_signal_test_metrics, action_signal_predict_train_metrics, action_signal_predict_test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_best_SVC\n",
    "###  - Inputs:\n",
    "        - model_type (str): toggles between selecting the best linear model or choosing from all types - valid inputs ('all', 'linear')\n",
    "        - alphas (list): list containing the distinct values of the regularization parameter to be chosen between\n",
    "        - folds (int): number of folds used for cross validation\n",
    "        - X_train (DataFrame): training feature data\n",
    "        - Y_train (DataFrame): training label data\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - best_model (estimator object): returns estimator selected based on the given parameters\n",
    "###  - Additional Notes:\n",
    "        - prints the parameters selected to generate the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_SVC(model_type, alphas, folds, X_train, Y_train):\n",
    "    if(model_type == 'linear'):\n",
    "        lin_svc = LinearSVC(dual=False ,max_iter=500000)\n",
    "        params = {'penalty':['l1','l2'],'C':alphas}\n",
    "        clf = GridSearchCV(estimator=lin_svc, param_grid=params, cv=folds)\n",
    "        clf.fit(X_train,Y_train)\n",
    "        \n",
    "        print(\"Best Parameters: \")\n",
    "        print(clf.best_params_)\n",
    "        \n",
    "        best_model = clf.best_estimator_\n",
    "    elif(model_type == 'all'):\n",
    "        svc = SVC(max_iter=500000)\n",
    "        params = {'C':alphas, 'kernel':['linear','poly','sigmoid','rbf']}\n",
    "        clf = GridSearchCV(estimator=svc, param_grid=params, cv=folds)\n",
    "        clf.fit(X_train,Y_train)\n",
    "        \n",
    "        print(\"Best Parameters: \")\n",
    "        print(clf.best_params_)\n",
    "        \n",
    "        best_model = clf.best_estimator_\n",
    "    else:\n",
    "        raise ValueError('Invalid Parameter Input - model_type')\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_best_knn_regressor\n",
    "###  - Inputs:\n",
    "        - lower_bound_k (int): lower bound of k values being selected from (inclusive)\n",
    "        - upper_bound_k (int): upper bound of k values being selected from (inclusive)\n",
    "        - step_size (int): step size between values of k in specified range [lower_bound_k, upper_bound_k]\n",
    "        - folds (int): number of folds used for cross validation\n",
    "        - X_train (DataFrame): training feature data\n",
    "        - Y_train (DataFrame): training label data\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - best_model (estimator object): returns estimator selected based on the given parameters\n",
    "###  - Additional Notes:\n",
    "        - prints the parameters selected to generate the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_knn_regressor(lower_bound_k, upper_bound_k, step_size, folds, X_train, Y_train):\n",
    "    #Create List of Possible K Values\n",
    "    k_vals = list()\n",
    "    while(lower_bound_k <= upper_bound_k):\n",
    "        k_vals.append(lower_bound_k)\n",
    "        lower_bound_k+=step_size\n",
    "    \n",
    "    #Define Parameters to Cross-Validate\n",
    "    params = {'n_neighbors':k_vals,'weights':['uniform','distance'],'p':[1,2,3]}\n",
    "    \n",
    "    #Select Best Parameters\n",
    "    knn = KNeighborsRegressor()\n",
    "\n",
    "    knn_reg = GridSearchCV(estimator=knn, param_grid=params, cv=folds)\n",
    "    knn_reg.fit(X_train,Y_train)\n",
    "        \n",
    "    print(\"Best Parameters: \")\n",
    "    print(knn_reg.best_params_)\n",
    "        \n",
    "    best_model = knn_reg.best_estimator_\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_best_SVR\n",
    "###  - Inputs:\n",
    "        - model_type (str): toggles between selecting the best linear model or choosing from all types - valid inputs ('all', 'linear')\n",
    "        - alphas (list): list containing the distinct values of the regularization parameter to be chosen between\n",
    "        - degrees (list) - different degrees of the polynomial kernel function to be chosen from if 'poly' is selected. Ignored by all other kernels \n",
    "        - folds (int): number of folds used for cross validation\n",
    "        - X_train (DataFrame): training feature data\n",
    "        - Y_train (DataFrame): training label data\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - best_model (estimator object): returns estimator selected based on the given parameters\n",
    "###  - Additional Notes:\n",
    "        - prints the parameters selected to generate the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_SVR(model_type, alphas, degrees, folds, X_train, Y_train):\n",
    "    if(model_type == 'linear'):\n",
    "        lin_svr = LinearSVR(dual = False, max_iter=500000)\n",
    "        params = {'loss':['squared_epsilon_insensitive'],'C':alphas, 'fit_intercept': [True, False]}\n",
    "        clf = GridSearchCV(estimator=lin_svr, param_grid=params, cv=folds)\n",
    "        clf.fit(X_train,Y_train)\n",
    "        \n",
    "        print(\"Best Parameters: \")\n",
    "        print(clf.best_params_)\n",
    "        \n",
    "        best_model = clf.best_estimator_\n",
    "    elif(model_type == 'all'):\n",
    "        svr = SVR(max_iter=500000)\n",
    "        params = {'C':alphas, 'kernel':['linear','poly','sigmoid','rbf'], 'degree': degrees }   #'gamma': ['scale','auto']\n",
    "        clf = GridSearchCV(estimator=svr, param_grid=params, cv=folds)\n",
    "        clf.fit(X_train,Y_train)\n",
    "        \n",
    "        print(\"Best Parameters: \")\n",
    "        print(clf.best_params_)\n",
    "        print(\"Ignore degree value if kernel is not 'poly' \")\n",
    "        best_model = clf.best_estimator_\n",
    "    else:\n",
    "        raise ValueError('Invalid Parameter Input - model_type')\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trader_single_stock\n",
    "###  - Inputs:\n",
    "        - predictions (numpy array): array of label predictions\n",
    "        - period (int): time in days of when we are predicting the return for\n",
    "        - metric (str): the label of the price being compared ex. 'close'\n",
    "        - numerical_data (DataFrame): use output of return_over_period_T function\n",
    "        - capital (int): initial amount of money for simulation\n",
    "        - index (int) start index of test set\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - capital (int): amount of money we have at the end of the testing period\n",
    "###  - Additional Notes:\n",
    "        - to make the function more modular we can use len(X_test) for index inputs until there is a better solution for slicing (would be nice to eliminate this parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trader_single_stock(predictions, period, metric, numerical_data, capital, index):\n",
    "    target = metric.lower() + '_return'\n",
    "    \n",
    "    r = numerical_data[target]\n",
    "    price = numerical_data[metric.upper()]\n",
    "    \n",
    "    labels = r.iloc[index:].values\n",
    "    price_labels = price.iloc[index:].values\n",
    "    \n",
    "    action = deque()\n",
    "    investment = deque()\n",
    "    \n",
    "    for i in range (len(predictions)):\n",
    "        #assume we always invest 1/3 of our available cash\n",
    "        cash = capital/3\n",
    "        \n",
    "        #too poor to buy stock :(\n",
    "        if(cash < price[i]):\n",
    "            continue\n",
    "\n",
    "        investment.append(cash)\n",
    "        capital-=cash\n",
    "        \n",
    "        if(i < period):\n",
    "            action.append(predictions[i])\n",
    "        else:\n",
    "            #make new prediction for t+T\n",
    "            action.append(predictions[i])\n",
    "        \n",
    "            #calculate return based on prediction\n",
    "            prediction = action.popleft()\n",
    "            past_investment = investment.popleft()\n",
    "            \n",
    "            if(prediction == 1):\n",
    "                #calculate long return\n",
    "                delta = labels[i-3]\n",
    "            elif(prediction == -1):\n",
    "                #calculate short return\n",
    "                delta = -labels[i-3]\n",
    "            else:\n",
    "                #did not invest T days ago\n",
    "                delta = 0\n",
    "            #calculate profit and update capital value\n",
    "            shares = past_investment/price_labels[i-3]\n",
    "            profit = (delta+1)*past_investment - 0.0075*shares\n",
    "            capital+=profit\n",
    "        #print(capital)\n",
    "        \n",
    "    for i in range(period):\n",
    "        capital+=investment.popleft()\n",
    "        \n",
    "    return capital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trade_portfolio\n",
    "\n",
    "###  - Inputs:\n",
    "        - stock_portofolio: list of stock ticker names that will be traded\n",
    "        - algo_of_choice: set to default as Linear SVM for now, will be integrated for any algo in the future\n",
    "        - capital: amount of capital to invest, will be distributed evenly amongst the stocks\n",
    "        - metric (str): the label of the price being compared ex. 'close'\n",
    "        - num_features: number of chi squared features to use\n",
    "        - period: period of T day return\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - all_company_capital (int): amount of money we have at the end of the testing period for each stock in our portfolio\n",
    "###  - Additional Notes:\n",
    "        - will be integrated to run any algo in future updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trade_portfolio(stock_portfolio, algo_of_choice, capital, metric, num_features, period ):\n",
    "    \n",
    "    num_stocks = len(stock_portfolio)\n",
    "    # all_company_returns = {}\n",
    "    # all_company_preds = {}                   \n",
    "    all_company_capital = {}\n",
    "    \n",
    "    for stock in stock_portfolio:\n",
    "    \n",
    "        file_name = stock + '.json'\n",
    "        csv_name = stock + '.csv'\n",
    "        #Features\n",
    "        company = stock_twits_text_parser(file_name)\n",
    "        company_metrics = stock_twits_metrics(company, period, file_name)\n",
    "\n",
    "        #Labels\n",
    "        company_return = return_over_period_T(csv_name,period)\n",
    "\n",
    "\n",
    "        #Cleaned Data\n",
    "        agg_data = feature_selector(company_metrics, company_return, 'close_return', 25, num_features)\n",
    "        company_data = split_data(agg_data, 0.7, 'all', 'close_return', 'bin')\n",
    "\n",
    "        #Split\n",
    "        X_train = company_data['X_train']\n",
    "        Y_train = company_data['Y_train']\n",
    "        X_test = company_data['X_test']\n",
    "        Y_test = company_data['Y_test']\n",
    "\n",
    "        #Define Values for Regularization Parameter\n",
    "        reg_vals = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4, 1e5]\n",
    "        \n",
    "        if algo_of_choice  == 'Linear SVM':\n",
    "            #Find Best Model\n",
    "            clf_lin_svc =  get_best_SVC('linear', reg_vals, 5, X_train, Y_train)\n",
    "\n",
    "            #Predict\n",
    "            pred_test = clf_lin_svc.predict(X_test)\n",
    "\n",
    "        #all_company_returns[stock] = company_return\n",
    "        #all_company_preds[stock] = pred_test\n",
    "        all_company_capital[stock] = trader_single_stock(pred_test, period, metric, company_return, capital/num_stocks, len(X_train))\n",
    "    \n",
    "    return all_company_capital\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name = 'FB.json'\n",
    "csv_name = 'FB.csv'\n",
    "\n",
    "company = stock_twits_text_parser(file_name)\n",
    "\n",
    "company_metrics = stock_twits_metrics(company, 3, file_name)\n",
    "company_return = return_over_period_T(csv_name,3)\n",
    "agg_data = feature_selector(company_metrics, company_return, 'close_return', 25, 500)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#company_data = split_data(agg_data, 0.7, 'all', 'close_return', 'bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>OPEN</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>LOW</th>\n",
       "      <th>VOLUME</th>\n",
       "      <th>CLOSE</th>\n",
       "      <th>open_return</th>\n",
       "      <th>high_return</th>\n",
       "      <th>low_return</th>\n",
       "      <th>close_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-07-23</td>\n",
       "      <td>96.959999</td>\n",
       "      <td>97.449997</td>\n",
       "      <td>94.809998</td>\n",
       "      <td>29418800</td>\n",
       "      <td>95.440002</td>\n",
       "      <td>-0.021865</td>\n",
       "      <td>-0.019395</td>\n",
       "      <td>-0.015821</td>\n",
       "      <td>-0.001572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-07-24</td>\n",
       "      <td>97.349998</td>\n",
       "      <td>97.760002</td>\n",
       "      <td>95.879997</td>\n",
       "      <td>33444900</td>\n",
       "      <td>96.949997</td>\n",
       "      <td>-0.010580</td>\n",
       "      <td>-0.004910</td>\n",
       "      <td>-0.011890</td>\n",
       "      <td>0.000413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-07-27</td>\n",
       "      <td>96.580002</td>\n",
       "      <td>96.610001</td>\n",
       "      <td>93.830002</td>\n",
       "      <td>38585400</td>\n",
       "      <td>94.169998</td>\n",
       "      <td>-0.017291</td>\n",
       "      <td>-0.008281</td>\n",
       "      <td>-0.021635</td>\n",
       "      <td>0.011044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-07-28</td>\n",
       "      <td>94.839996</td>\n",
       "      <td>95.559998</td>\n",
       "      <td>93.309998</td>\n",
       "      <td>35236000</td>\n",
       "      <td>95.290001</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>0.008058</td>\n",
       "      <td>0.006859</td>\n",
       "      <td>-0.013433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-07-29</td>\n",
       "      <td>96.320000</td>\n",
       "      <td>97.279999</td>\n",
       "      <td>94.739998</td>\n",
       "      <td>64648300</td>\n",
       "      <td>96.989998</td>\n",
       "      <td>-0.028966</td>\n",
       "      <td>-0.022615</td>\n",
       "      <td>-0.020477</td>\n",
       "      <td>-0.029384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>2016-11-22</td>\n",
       "      <td>122.400002</td>\n",
       "      <td>122.980003</td>\n",
       "      <td>120.900002</td>\n",
       "      <td>25992700</td>\n",
       "      <td>121.470001</td>\n",
       "      <td>-0.018627</td>\n",
       "      <td>-0.010490</td>\n",
       "      <td>-0.008933</td>\n",
       "      <td>-0.008726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>2016-11-23</td>\n",
       "      <td>121.230003</td>\n",
       "      <td>121.309998</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>15592400</td>\n",
       "      <td>120.839996</td>\n",
       "      <td>-0.005444</td>\n",
       "      <td>0.006512</td>\n",
       "      <td>0.003835</td>\n",
       "      <td>0.000248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>2016-11-25</td>\n",
       "      <td>121.010002</td>\n",
       "      <td>121.139999</td>\n",
       "      <td>120.070000</td>\n",
       "      <td>8638400</td>\n",
       "      <td>120.379997</td>\n",
       "      <td>-0.005702</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>-0.017656</td>\n",
       "      <td>-0.016282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>2016-11-28</td>\n",
       "      <td>120.120003</td>\n",
       "      <td>121.690002</td>\n",
       "      <td>119.820000</td>\n",
       "      <td>18073300</td>\n",
       "      <td>120.410004</td>\n",
       "      <td>-0.014486</td>\n",
       "      <td>-0.026625</td>\n",
       "      <td>-0.048573</td>\n",
       "      <td>-0.044099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>2016-11-29</td>\n",
       "      <td>120.570000</td>\n",
       "      <td>122.099998</td>\n",
       "      <td>120.400002</td>\n",
       "      <td>18846600</td>\n",
       "      <td>120.870003</td>\n",
       "      <td>-0.045285</td>\n",
       "      <td>-0.046028</td>\n",
       "      <td>-0.050664</td>\n",
       "      <td>-0.045255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>343 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        OPEN        HIGH         LOW    VOLUME       CLOSE  \\\n",
       "0    2015-07-23   96.959999   97.449997   94.809998  29418800   95.440002   \n",
       "1    2015-07-24   97.349998   97.760002   95.879997  33444900   96.949997   \n",
       "2    2015-07-27   96.580002   96.610001   93.830002  38585400   94.169998   \n",
       "3    2015-07-28   94.839996   95.559998   93.309998  35236000   95.290001   \n",
       "4    2015-07-29   96.320000   97.279999   94.739998  64648300   96.989998   \n",
       "..          ...         ...         ...         ...       ...         ...   \n",
       "338  2016-11-22  122.400002  122.980003  120.900002  25992700  121.470001   \n",
       "339  2016-11-23  121.230003  121.309998  119.940002  15592400  120.839996   \n",
       "340  2016-11-25  121.010002  121.139999  120.070000   8638400  120.379997   \n",
       "341  2016-11-28  120.120003  121.690002  119.820000  18073300  120.410004   \n",
       "342  2016-11-29  120.570000  122.099998  120.400002  18846600  120.870003   \n",
       "\n",
       "     open_return  high_return  low_return  close_return  \n",
       "0      -0.021865    -0.019395   -0.015821     -0.001572  \n",
       "1      -0.010580    -0.004910   -0.011890      0.000413  \n",
       "2      -0.017291    -0.008281   -0.021635      0.011044  \n",
       "3       0.001160     0.008058    0.006859     -0.013433  \n",
       "4      -0.028966    -0.022615   -0.020477     -0.029384  \n",
       "..           ...          ...         ...           ...  \n",
       "338    -0.018627    -0.010490   -0.008933     -0.008726  \n",
       "339    -0.005444     0.006512    0.003835      0.000248  \n",
       "340    -0.005702     0.005366   -0.017656     -0.016282  \n",
       "341    -0.014486    -0.026625   -0.048573     -0.044099  \n",
       "342    -0.045285    -0.046028   -0.050664     -0.045255  \n",
       "\n",
       "[343 rows x 10 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
