{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import Required Packages\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define File Paths as Constants\n",
    "TWITS_PATH = r\"../stocktwits_prediction/Data/twits\"\n",
    "FINANCE_PATH = r\"../stocktwits_prediction/Data/csv\"\n",
    "\n",
    "#Define Company Names and List of Tags as Constant\n",
    "COMPANY_NAMES = [\"apple\",\"boeing\", \"caterpillar\", \"cisco\", \"chevron\", \"dupont\", \"de\", \"nemours\", \"walt\", \n",
    "                 \"diseny\", \"facebook\", \"google\", \"vaneck\", \"goldman\", \"sachs\", \"ibm\", \"intel\", \"johnson\",\n",
    "                \"jpmorgan\", \"coca\", \"cola\", \"coca-cola\", \"mcdonalds\", \"3m\", \"merck\", \"microsoft\", \"nike\",\n",
    "                \"pfizer\", \"unitedhealth\", \"raytheon\", \"visa\", \"verizon\", \"walmart\", \n",
    "                 \"exxon\", \"mobil\"]\n",
    "\n",
    "TAGS = ['\\$FB','\\$GOOG','\\$AAPL', '\\$WB','\\$AXP','\\$BA','\\$CAT','\\$CSCO','\\$CVX','\\$DD','\\$DIS','\\$FB',\n",
    "        '\\$GE','\\$GS','\\$HD','\\$IBM','\\$INTC','\\$JNJ','\\$JPM','\\$KO','\\$MCD','\\$MMM','\\$MRK','\\$MSFT',\n",
    "        '\\$NKE','\\$PFE','\\$PG','\\$QQQ','\\$SPY','\\$TRV','\\$UNH','\\$UTX','\\$V','\\$VZ','\\$WMT','\\$XOM']\n",
    "\n",
    "LABELS = [\"Positive Return\", \"Negative Return\"]\n",
    "\n",
    "#Return ALl Files in Directory\n",
    "all_twitter = os.listdir(TWITS_PATH)\n",
    "all_finance = os.listdir(FINANCE_PATH)\n",
    "\n",
    "#Generate List of Tags\n",
    "all_tags = list()\n",
    "for file_name in all_twitter:\n",
    "    tag = '$' + file_name[:-4]\n",
    "    all_tags.append(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stock_twits_metrics\n",
    "###  - Inputs:\n",
    "        - raw_data: DataFrame (can use returned DataFrame from the text parser\n",
    "        - n (int): number of days (used for s_t calculation)\n",
    "        - file_name (str): name of file - ex. 'FB.txt'\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - Date: DateTime object representing the a specific date in the sample period\n",
    "        - bearish: total count of tweets tagged with bearish sentiment on that day\n",
    "        - bullish: total count of tweets tagged with bullish sentiment on that day\n",
    "        - none: total count of tweets tagged with neither bearish nor bullisn sentiment on that day\n",
    "        - message_volume: total number of messages on that day \n",
    "        - mv1_t:  percentage change in message volume with one day period\n",
    "        - mv10_t: percentage change in message volume with ten day period\n",
    "        - polarity: (bullish - bearish)/total messages on that day\n",
    "        - s_t: moving average of polarity over n days\n",
    "        - company: string containing the abbreviation of the company name\n",
    "###  - Additional Notes:\n",
    "        - function aggregates the data into values by date - does not include the text of the orignal tweets in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_twits_metrics(raw_data, n, file_name):\n",
    "    #Unwrap Sentiment Values to New Columns\n",
    "    raw_data['sentiment'] = raw_data['sentiment'].apply(lambda x:{} if pd.isna(x) else x)\n",
    "    sentiment_vals = json_normalize(data=raw_data['sentiment'], meta=['class','name'])\n",
    "    sentiment_vals = sentiment_vals.drop(columns='name')\n",
    "    sentiment_vals = sentiment_vals.replace(np.nan, 'none', regex=True)\n",
    "\n",
    "    #Convert Date Strings to DateTime Objects\n",
    "    raw_data['created_at'] = pd.to_datetime(raw_data['created_at']).apply(lambda x: x.date())\n",
    "\n",
    "    #Create New Dataframe\n",
    "    temp = pd.concat([raw_data['body'], sentiment_vals, raw_data['created_at']],axis=1)\n",
    "    \n",
    "    #Aggregate Text From Tweets By Date\n",
    "    agg_text = temp.groupby('created_at')['body'].apply(lambda x: x.sum())\n",
    "    agg_text.to_frame()\n",
    "    \n",
    "    #Group by Unique Dates and Extract Value Counts\n",
    "    message_volume = temp['created_at'].value_counts().rename_axis('dates').reset_index(name='message_volume')\n",
    "    message_volume = message_volume.sort_values(by='dates').reset_index(drop=True)\n",
    "\n",
    "    #Create Pivot Table With Desired Statistics\n",
    "    temp['count'] = 1\n",
    "    table = temp.pivot_table(index=['created_at'], dropna=False,\n",
    "                             columns='class',values='count',\n",
    "                             fill_value=0,aggfunc=np.sum)\n",
    "    \n",
    "    #Merge Table with Message Volume\n",
    "    table = table.reset_index()\n",
    "    table = pd.concat([table,message_volume['message_volume']],axis=1)\n",
    "\n",
    "    #Calculate Metrics and Append to the DataFrame\n",
    "    table['mv1_t'] = table['message_volume'].diff(periods=1).div(table['message_volume'].shift(1))\n",
    "    table['mv10_t'] = table['message_volume'].div(table['message_volume'].rolling(10).mean())\n",
    "\n",
    "    table['polarity'] = table.apply(lambda date: (date.bullish - date.bearish)/date.message_volume, axis = 1)\n",
    "    table['s_t'] = table['polarity'].rolling(n).mean()\n",
    "    \n",
    "    #Append Company Name to DataFrame\n",
    "    name = file_name[:-4]\n",
    "    table['company'] = name\n",
    "    \n",
    "    #Append Text to DataFrame\n",
    "    result = pd.merge(table, agg_text, on='created_at')\n",
    "    \n",
    "    #Change Column Name to Date\n",
    "    result = result.rename(columns={\"created_at\":\"Date\"})\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stock_twits_text_parser\n",
    "###  - Inputs:\n",
    "        - file (str): name of file - ex. 'FB.txt'\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - df: DataFrame with all rows containing multiple tagged companies dropped, all columns included - index reset to reflect new size\n",
    "###  - Additional Notes:\n",
    "        - None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_twits_text_parser(file):\n",
    "    #Read File\n",
    "    file_path = TWITS_PATH + '/' + file\n",
    "    raw_data = pd.read_json(file_path)\n",
    "\n",
    "    #Create New Column with List of Unique Tags\n",
    "    raw_data[\"instances\"] = (raw_data['body'].str.findall(f'(?i)({\"|\".join(TAGS)})')\n",
    "                                .apply(lambda x: list(set(map(str.upper,x))))\n",
    "                            )\n",
    "    #Drop All Rows With Multiple Tags and Reset Index\n",
    "    df = raw_data[raw_data['instances'].map(lambda x: len(x)) < 2]\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature_selector\n",
    "###  - Inputs:\n",
    "        - metrics (DataFrame): DataFrame containing the body text of the tweet eg. data['body'] along with other metrics\n",
    "        - labels (DataFrame) DataFrame containing the target labels\n",
    "        - target_label (str): name of the column containing the target label we want to use\n",
    "        - min_count(int) : minimum number of occurances so that the word is converted\n",
    "        - n (int): number of text features selected\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - df: DataFrame with top n text features and all numerical features, raw data and labels (20 + n columns total)\n",
    "###  - Additional Notes:\n",
    "        - text features will always be appended after the numerical metrics, raw data and labels - currently these are the first 20 columns: use df.iloc[:,20:] to extract only text features for analysis\n",
    "        - text features selected using chi squared statistics\n",
    "        - ***TODO: Currently the method selects features based on categorical outcomes (classification problem) this will need to be changed to allow for regression (explore a way to bin continuous target data) ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selector(metrics, labels, target_label, min_count, n):\n",
    "    #Merge DataFranes\n",
    "    result = pd.merge(metrics, labels, on='Date')\n",
    "    \n",
    "    #Remove Company Names By Adding Them To Stop Words\n",
    "    new_stop_words = text.ENGLISH_STOP_WORDS.union(COMPANY_NAMES)\n",
    "\n",
    "    #Vectorizer Parameters: Convert To Lowercase, Remove Stop Words With Company Names,Min Count 25, Laplace Smoothing\n",
    "    v = TfidfVectorizer(analyzer='word', lowercase=True,stop_words=new_stop_words, min_df = min_count, smooth_idf=True)\n",
    "    x = v.fit_transform(result['body'])\n",
    "\n",
    "    #Convert back to DataFrame\n",
    "    text_data = pd.DataFrame(x.toarray(),columns=v.get_feature_names())\n",
    "    \n",
    "    #Select n Best Features\n",
    "    X = text_data\n",
    "    Y = convert_to_binary_classification(result[target_label])\n",
    "    chi2_selector = SelectKBest(chi2, k=n)\n",
    "    X_best = chi2_selector.fit_transform(X,Y)\n",
    "    \n",
    "    data = pd.DataFrame(X_best)\n",
    "    df = pd.concat([result,data],axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# return_over_period_T\n",
    "###  - Inputs:\n",
    "        - file (str): name of file - ex. 'FB.csv'\n",
    "        - T (int): period based on which return is calculated (in days)\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - Date: DateTime object representing a specific date in the sample period\n",
    "        - OPEN, HIGH, LOW, VOLUME, CLOSE: prices each day with respect to the specific metric\n",
    "        - xxx_return: return for referenced metric over period T, where T is specified when the function is called\n",
    "###  - Additional Notes:\n",
    "        - function includes original data taken from the CSV file\n",
    "        - formula for return over period T is defined as: r(T) = (p(t+T) - p(t))/p(t), where t is current day and p(t) i the price at the current day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_over_period_T(file, T):\n",
    "    #Read File\n",
    "    file_path = FINANCE_PATH + '/' + file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    #Convert Date Strings to DateTime Objects\n",
    "    df['Date'] = pd.to_datetime(df['Date']).apply(lambda x: x.date())\n",
    "\n",
    "    #Calculate Metrics for OPEN, HIGH, LOW, VOLUME and CLOSE\n",
    "    df['open_return'] = -df['OPEN'].diff(periods = -T).div(df['OPEN'])\n",
    "    df['high_return'] = -df['HIGH'].diff(periods = -T).div(df['HIGH'])\n",
    "    df['low_return'] = -df['LOW'].diff(periods = -T).div(df['LOW'])\n",
    "    df['close_return'] = -df['CLOSE'].diff(periods = -T).div(df['CLOSE'])\n",
    "    \n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert_to_binary_classification\n",
    "###  - Inputs:\n",
    "        - df (DataFrame): single column to be classified\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - Binary Labels: single column of binary labels representing positive and negative returns on the day\n",
    "###  - Additional Notes:\n",
    "        - Note that a negative return is defined as class 0 and a postive return or no return is defined as class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_binary_classification(df):\n",
    "    binary_classification = df.apply(lambda x: 0 if x < 0 else 1)\n",
    "    return binary_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split_data\n",
    "###  - Inputs:\n",
    "        - df (DataFrame): Processed DataFrame with features and labels to be split\n",
    "        - train_percentage (float): Percentage of data to be used as training data - assumes all other data is used as test\n",
    "        - features (str): toggles whether to use text only or text and numerical features (valid inputs: 'text', 'all')\n",
    "        - label (str): selects target label based on column name\n",
    "        - target_type (str): binary (bin) or continuous (cont)\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - data_dict (Dictionary): dictionary with 4 values corresponding to X_train, X_test, Y_train, Y_test (keys)\n",
    "###  - Additional Notes:\n",
    "        - ***TODO: does not handle continous data correctly***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, train_percentage, features, label, target_type):\n",
    "    #Determine Split Index Based on the Percentage Of Data to Be Used For Training\n",
    "    train_index = int(train_percentage*len(df.index)) + 1\n",
    "    \n",
    "    #Split Data\n",
    "    if(features == 'text'):\n",
    "        X_train = df.iloc[0:train_index,20:]\n",
    "        X_test = df.iloc[train_index:,20:]\n",
    "    elif(features == 'all'):\n",
    "        df1 = df.iloc[0:train_index:,np.r_[5:7,8]]\n",
    "        df2 = df.iloc[0:train_index,20:]\n",
    "        X_train = pd.concat([df1,df2],axis=1)\n",
    "        \n",
    "        df3 = df.iloc[train_index:,np.r_[5:7,8]]\n",
    "        df4 = df.iloc[train_index:,20:]\n",
    "        X_test = pd.concat([df3,df4],axis=1)\n",
    "    else:\n",
    "        raise ValueError('Invalid Parameter Input - features')\n",
    "        \n",
    "    if(target_type == 'bin' or target_type == 'cont'):\n",
    "        if(target_type == 'bin'):\n",
    "            Y = convert_to_binary_classification(df[label])\n",
    "        Y_train = Y.iloc[0:train_index]\n",
    "        Y_test = Y.iloc[train_index:]\n",
    "    else:\n",
    "        raise ValueError('Invalid Parameter Input - target_type')\n",
    "    \n",
    "    #Return as a Dictionary\n",
    "    data_dict = dict()\n",
    "    \n",
    "    data_dict['X_train'] = X_train\n",
    "    data_dict['X_test'] = X_test\n",
    "    data_dict['Y_train'] = Y_train\n",
    "    data_dict['Y_test'] = Y_test\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create_metrics_table\n",
    "###  - Inputs:\n",
    "        - Y_train: training labels\n",
    "        - pred_train: predicted labels from training data\n",
    "        - Y_test: training labels\n",
    "        - pred_test: predicted labels from test data\n",
    "        - valid input types (series, numpy array, DF columns) - must be column vectors\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - table (DataFrame): table with REC, PREC, F1 amd ACC for training and test predictions\n",
    "###  - Additional Notes:\n",
    "        - None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_table(Y_train, pred_train, Y_test, pred_test):\n",
    "    #Calculate Metrics\n",
    "    REC_train = recall_score(Y_train,pred_train)\n",
    "    PREC_train = precision_score(Y_train, pred_train)\n",
    "    F1_train = f1_score(Y_train, pred_train)\n",
    "    ACC_train = accuracy_score(Y_train, pred_train)\n",
    "\n",
    "    REC_test = recall_score(Y_test,pred_test)\n",
    "    PREC_test = precision_score(Y_test,pred_test)\n",
    "    F1_test = f1_score(Y_test, pred_test)\n",
    "    ACC_test = accuracy_score(Y_test, pred_test)\n",
    "\n",
    "    #Create Table\n",
    "    rows = [[\"Training\", REC_train, PREC_train, F1_train, ACC_train], [\"Test\", REC_test, PREC_test, F1_test,ACC_test]]\n",
    "    table = pd.DataFrame(rows, columns = [\"Dataset\", \"Recall\", \"Precision\", \"F1 Score\", \"Accuracy\"])\n",
    "    table.set_index(\"Dataset\", inplace =True)\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot_confusion_matrix\n",
    "###  - Inputs:\n",
    "        - matrix (confusion matrix): confusion matrix generated from sklearn confusion_matrix\n",
    "        - data_set_name (str): name of dataset ex. 'training' or 'test' \n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - None - void type function\n",
    "        - Displays plot\n",
    "###  - Additional Notes:\n",
    "        - Wrapper for seaborn plot code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(matrix, data_set_name):\n",
    "    sns.heatmap(matrix, annot=True, fmt= \".3f\", xticklabels = LABELS, yticklabels = LABELS)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"Actual Label\")\n",
    "    plt.title(\"Confusion Matrix for \" + data_set_name + \" Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot_ROC_curve\n",
    "###  - Inputs:\n",
    "        - FPR: False Positive Rate - Generated from sklearn roc_curve\n",
    "        - TPR: True Positive Rate - Generated from sklearn roc_curve\n",
    "        - data_set_name (str): name of dataset ex. 'training' or 'test'\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - None - void type function\n",
    "        - Displays plot\n",
    "###  - Additional Notes:\n",
    "        - Wrapper for matplotlib plot code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC_curve(FPR, TPR, data_set_name):\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "    plt.plot(FPR, TPR)\n",
    "    plt.xlabel(\"False Positive\")\n",
    "    plt.ylabel(\"True Positive\")\n",
    "    plt.title(\"ROC Curve for \" + data_set_name + \" Data\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_best_SVC\n",
    "###  - Inputs:\n",
    "        - model_type (str): toggles between selecting the best linear model or choosing from all types - valid inputs ('all', 'linear')\n",
    "        - alphas (list): list containing the distinct values of the regularization parameter to be chosen between\n",
    "        - folds (int): number of folds used for cross validation\n",
    "        - X_train (DataFrame): training feature data\n",
    "        - Y_train (DataFrame): training label data\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - best_model (estimator object): returns estimator selected based on the given parameters\n",
    "###  - Additional Notes:\n",
    "        - prints the parameters selected to generate the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_SVC(model_type, alphas, folds, X_train, Y_train):\n",
    "    if(model_type == 'linear'):\n",
    "        lin_svc = LinearSVC(dual=False ,max_iter=500000)\n",
    "        params = {'penalty':['l1','l2'],'C':alphas}\n",
    "        clf = GridSearchCV(estimator=lin_svc, param_grid=params, cv=folds)\n",
    "        clf.fit(X_train,Y_train)\n",
    "        \n",
    "        print(\"Best Parameters: \")\n",
    "        print(clf.best_params_)\n",
    "        \n",
    "        best_model = clf.best_estimator_\n",
    "    elif(model_type == 'all'):\n",
    "        svc = SVC(max_iter=500000)\n",
    "        params = {'C':alphas, 'kernel':['linear','poly','sigmoid','rbf']}\n",
    "        clf = GridSearchCV(estimator=svc, param_grid=params, cv=folds)\n",
    "        clf.fit(X_train,Y_train)\n",
    "        \n",
    "        print(\"Best Parameters: \")\n",
    "        print(clf.best_params_)\n",
    "        \n",
    "        best_model = clf.best_estimator_\n",
    "    else:\n",
    "        raise ValueError('Invalid Parameter Input - model_type')\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = 'FB.txt'\n",
    "#csv_name = 'FB.csv'\n",
    "\n",
    "#company = stock_twits_text_parser(file_name)\n",
    "#company_metrics = stock_twits_metrics(company, 3, file_name)\n",
    "#company_return = return_over_period_T(csv_name,3)\n",
    "#agg_data = feature_selector(company_metrics, company_return, 'close_return', 25, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#company_data = split_data(agg_data, 0.7, 'all', 'close_return', 'bin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
