{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import Required Packages\n",
    "import json\n",
    "import os\n",
    "import logging as log\n",
    "import requests\n",
    "#import yfinance as yf\n",
    "import time as time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from pandas.io.json import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "import keras\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define File Paths as Constants\n",
    "#TWITS_PATH = r\"../stocktwits_prediction/Data/twits\"\n",
    "#FINANCE_PATH = r\"../stocktwits_prediction/Data/csv\"\n",
    "TWITS_PATH = \"../Data/twits\"\n",
    "FINANCE_PATH = \"../Data/csv\"\n",
    "#Define Company Names and List of Tags as Constant\n",
    "COMPANY_NAMES = [\"apple\",\"boeing\", \"caterpillar\", \"cisco\", \"chevron\", \"dupont\", \"de\", \"nemours\", \"walt\", \n",
    "                 \"diseny\", \"facebook\", \"google\", \"vaneck\", \"goldman\", \"sachs\", \"ibm\", \"intel\", \"johnson\",\n",
    "                \"jpmorgan\", \"coca\", \"cola\", \"coca-cola\", \"mcdonalds\", \"3m\", \"merck\", \"microsoft\", \"nike\",\n",
    "                \"pfizer\", \"unitedhealth\", \"raytheon\", \"visa\", \"verizon\", \"walmart\", \n",
    "                 \"exxon\", \"mobil\"]\n",
    "\n",
    "TAGS = ['\\$FB','\\$GOOG','\\$AAPL', '\\$WB','\\$AXP','\\$BA','\\$CAT','\\$CSCO','\\$CVX','\\$DD','\\$DIS','\\$FB',\n",
    "        '\\$GE','\\$GS','\\$HD','\\$IBM','\\$INTC','\\$JNJ','\\$JPM','\\$KO','\\$MCD','\\$MMM','\\$MRK','\\$MSFT',\n",
    "        '\\$NKE','\\$PFE','\\$PG','\\$QQQ','\\$SPY','\\$TRV','\\$UNH','\\$UTX','\\$V','\\$VZ','\\$WMT','\\$XOM']\n",
    "\n",
    "LABELS = [\"Positive Return\", \"Negative Return\"]\n",
    "\n",
    "#Return ALl Files in Directory\n",
    "all_twitter = os.listdir(TWITS_PATH)\n",
    "all_finance = os.listdir(FINANCE_PATH)\n",
    "\n",
    "#Generate List of Tags\n",
    "all_tags = list()\n",
    "for file_name in all_twitter:\n",
    "    tag = '$' + file_name[:-4]\n",
    "    all_tags.append(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stock_twits_metrics\n",
    "###  - Inputs:\n",
    "        - raw_data: DataFrame (can use returned DataFrame from the text parser\n",
    "        - n (int): number of days (used for s_t calculation)\n",
    "        - file_name (str): name of file - ex. 'FB.txt'\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - Date: DateTime object representing the a specific date in the sample period\n",
    "        - bearish: total count of tweets tagged with bearish sentiment on that day\n",
    "        - bullish: total count of tweets tagged with bullish sentiment on that day\n",
    "        - none: total count of tweets tagged with neither bearish nor bullisn sentiment on that day\n",
    "        - message_volume: total number of messages on that day \n",
    "        - mv1_t:  percentage change in message volume with one day period\n",
    "        - mv10_t: percentage change in message volume with ten day period\n",
    "        - polarity: (bullish - bearish)/total messages on that day\n",
    "        - s_t: moving average of polarity over n days\n",
    "        - company: string containing the abbreviation of the company name\n",
    "###  - Additional Notes:\n",
    "        - function aggregates the data into values by date - does not include the text of the orignal tweets in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_twits_metrics(raw_data, n, file_name):\n",
    "    #Unwrap Sentiment Values to New Columns\n",
    "    raw_data['sentiment'] = raw_data['sentiment'].apply(lambda x:{} if pd.isna(x) else x)\n",
    "    sentiment_vals = pd.json_normalize(data=raw_data['sentiment'], meta=['class','name'])\n",
    "    sentiment_vals = sentiment_vals.drop(columns='name')\n",
    "    sentiment_vals = sentiment_vals.replace(np.nan, 'none', regex=True)\n",
    "\n",
    "    #Convert Date Strings to DateTime Objects\n",
    "    raw_data['created_at'] = pd.to_datetime(raw_data['created_at']).apply(lambda x: x.date())\n",
    "\n",
    "    #Create New Dataframe\n",
    "    temp = pd.concat([raw_data['body'], sentiment_vals, raw_data['created_at']],axis=1)\n",
    "    \n",
    "    #Aggregate Text From Tweets By Date\n",
    "    agg_text = temp.groupby('created_at')['body'].apply(lambda x: x.sum())\n",
    "    agg_text.to_frame()\n",
    "    \n",
    "    #Group by Unique Dates and Extract Value Counts\n",
    "    message_volume = temp['created_at'].value_counts().rename_axis('dates').reset_index(name='message_volume')\n",
    "    message_volume = message_volume.sort_values(by='dates').reset_index(drop=True)\n",
    "\n",
    "    #Create Pivot Table With Desired Statistics\n",
    "    temp['count'] = 1\n",
    "    table = temp.pivot_table(index=['created_at'], dropna=False,\n",
    "                             columns='class',values='count',\n",
    "                             fill_value=0,aggfunc=np.sum)\n",
    "    \n",
    "    #Merge Table with Message Volume\n",
    "    table = table.reset_index()\n",
    "    table = pd.concat([table,message_volume['message_volume']],axis=1)\n",
    "\n",
    "    #Calculate Metrics and Append to the DataFrame\n",
    "    table['mv1_t'] = table['message_volume'].diff(periods=1).div(table['message_volume'].shift(1))\n",
    "    table['mv10_t'] = table['message_volume'].div(table['message_volume'].rolling(10).mean())\n",
    "\n",
    "    table['polarity'] = table.apply(lambda date: (date.bullish - date.bearish)/date.message_volume, axis = 1)\n",
    "    table['s_t'] = table['polarity'].rolling(n).mean()\n",
    "    \n",
    "    #Append Company Name to DataFrame\n",
    "    name = file_name[:-4]\n",
    "    table['company'] = name\n",
    "    \n",
    "    #Append Text to DataFrame\n",
    "    result = pd.merge(table, agg_text, on='created_at')\n",
    "    \n",
    "    #Change Column Name to Date\n",
    "    result = result.rename(columns={\"created_at\":\"Date\"})\n",
    "    \n",
    "    result = result.dropna(how = 'any')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stock_twits_text_parser\n",
    "###  - Inputs:\n",
    "        - file (str): name of file - ex. 'FB.txt'\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - df: DataFrame with all rows containing multiple tagged companies dropped, all columns included - index reset to reflect new size\n",
    "###  - Additional Notes:\n",
    "        - None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_twits_text_parser(file):\n",
    "    #Read File\n",
    "    file_path = TWITS_PATH + '/' + file\n",
    "    raw_data = pd.read_json(file_path)\n",
    "\n",
    "    #Create New Column with List of Unique Tags\n",
    "    raw_data[\"instances\"] = (raw_data['body'].str.findall(f'(?i)({\"|\".join(TAGS)})')\n",
    "                                .apply(lambda x: list(set(map(str.upper,x))))\n",
    "                            )\n",
    "    #Drop All Rows With Multiple Tags and Reset Index\n",
    "    df = raw_data[raw_data['instances'].map(lambda x: len(x)) < 2]\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature_selector\n",
    "###  - Inputs:\n",
    "        - metrics (DataFrame): DataFrame containing the body text of the tweet eg. data['body'] along with other metrics\n",
    "        - labels (DataFrame) DataFrame containing the target labels\n",
    "        - target_label (str): name of the column containing the target label we want to use\n",
    "        - min_count(int) : minimum number of occurances so that the word is converted\n",
    "        - n (int): number of text features selected\n",
    "        - encoding (str): default or optimized (see convert_to_binary_classification)\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - df: DataFrame with top n text features and all numerical features, raw data and labels (20 + n columns total)\n",
    "###  - Additional Notes:\n",
    "        - text features will always be appended after the numerical metrics, raw data and labels - currently these are the first 20 columns: use df.iloc[:,20:] to extract only text features for analysis\n",
    "        - text features selected using chi squared statistics\n",
    "        - ***TODO: Currently the method selects features based on categorical outcomes (classification problem) this will need to be changed to allow for regression (explore a way to bin continuous target data) ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selector(metrics, labels, target_label, min_count, n, encoding):\n",
    "    #Merge DataFranes\n",
    "    result = pd.merge(metrics, labels, on='Date')\n",
    "    \n",
    "    #Remove Company Names By Adding Them To Stop Words\n",
    "    new_stop_words = text.ENGLISH_STOP_WORDS.union(COMPANY_NAMES)\n",
    "\n",
    "    #Vectorizer Parameters: Convert To Lowercase, Remove Stop Words With Company Names,Min Count 25, Laplace Smoothing\n",
    "    v = TfidfVectorizer(analyzer='word', lowercase=True,stop_words=new_stop_words, min_df = min_count, smooth_idf=True)\n",
    "    x = v.fit_transform(result['body'])\n",
    "\n",
    "    #Convert back to DataFrame\n",
    "    text_data = pd.DataFrame(x.toarray(),columns=v.get_feature_names())\n",
    "    \n",
    "    #Select n Best Features\n",
    "    X = text_data\n",
    "    if(X.shape[1] >= n):\n",
    "        Y = convert_to_binary_classification(result[target_label], encoding)\n",
    "        chi2_selector = SelectKBest(chi2, k=n)\n",
    "        X_best = chi2_selector.fit_transform(X,Y)\n",
    "    else:\n",
    "        X_best = X\n",
    "        print(\"Warning: found max of \" + str(X.shape[1]) + \" text features - all features used\")\n",
    "    \n",
    "    data = pd.DataFrame(X_best)\n",
    "    df = pd.concat([result,data],axis=1)\n",
    "    \n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# return_over_period_T\n",
    "###  - Inputs:\n",
    "        - file (str): name of file - ex. 'FB.csv'\n",
    "        - T (int): period based on which return is calculated (in days)\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - Date: DateTime object representing a specific date in the sample period\n",
    "        - OPEN, HIGH, LOW, VOLUME, CLOSE: prices each day with respect to the specific metric\n",
    "        - xxx_return: return for referenced metric over period T, where T is specified when the function is called\n",
    "###  - Additional Notes:\n",
    "        - function includes original data taken from the CSV file\n",
    "        - formula for return over period T is defined as: r(T) = (p(t+T) - p(t))/p(t), where t is current day and p(t) i the price at the current day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_over_period_T(file, T):\n",
    "    #Read File\n",
    "    file_path = FINANCE_PATH + '/' + file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    #Convert Date Strings to DateTime Objects\n",
    "    df['Date'] = pd.to_datetime(df['Date']).apply(lambda x: x.date())\n",
    "\n",
    "    #Calculate Metrics for OPEN, HIGH, LOW, VOLUME and CLOSE\n",
    "    df['open_return'] = -df['OPEN'].diff(periods = -T).div(df['OPEN'])\n",
    "    df['high_return'] = -df['HIGH'].diff(periods = -T).div(df['HIGH'])\n",
    "    df['low_return'] = -df['LOW'].diff(periods = -T).div(df['LOW'])\n",
    "    df['close_return'] = -df['CLOSE'].diff(periods = -T).div(df['CLOSE'])\n",
    "    \n",
    "    df = df.dropna(how='any')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# return_over_period_T_API\n",
    "###  - Inputs:\n",
    "        - stock_name (str): name of stock (abbreviation) ex. Microsoft = MSFT\n",
    "        - T (int): period based on which return is calculated (in days)\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - Date: DateTime object representing a specific date in the sample period\n",
    "        - OPEN, HIGH, LOW, VOLUME, CLOSE: prices each day with respect to the specific metric\n",
    "        - xxx_return: return for referenced metric over period T, where T is specified when the function is called\n",
    "###  - Additional Notes:\n",
    "        - function includes original data taken from the CSV file\n",
    "        - formula for return over period T is defined as: r(T) = (p(t+T) - p(t))/p(t), where t is current day and p(t) i the price at the current day\n",
    "        - public API is rate limited "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_over_period_T_API(stock_name, T):    \n",
    "    #Public api (w/o auth token) - limited to 2000 requests/IP\n",
    "    stock = yf.Ticker(stock_name)\n",
    "\n",
    "    #Stops rate limiting from blocking IP\n",
    "    time.sleep(1)\n",
    "\n",
    "    #Get historical market data\n",
    "    hist = stock.history(period=\"max\")\n",
    "\n",
    "    df = hist.reset_index()\n",
    "    df = df.drop(['Dividends', 'Stock Splits'], axis=1)\n",
    "\n",
    "    #Convert Date Strings to DateTime Objects\n",
    "    df['Date'] = pd.to_datetime(df['Date']).apply(lambda x: x.date())\n",
    "\n",
    "    #Calculate Metrics for OPEN, HIGH, LOW, VOLUME and CLOSE\n",
    "    df['open_return'] = -df['Open'].diff(periods = -T).div(df['Open'])\n",
    "    df['high_return'] = -df['High'].diff(periods = -T).div(df['High'])\n",
    "    df['low_return'] = -df['Low'].diff(periods = -T).div(df['Low'])\n",
    "    df['close_return'] = -df['Close'].diff(periods = -T).div(df['Close'])\n",
    "    \n",
    "    df = df.dropna(how='any')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert_to_binary_classification\n",
    "###  - Inputs:\n",
    "        - df (DataFrame): single column to be classified\n",
    "        - encoding (str): tells function which encoding to use\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - Binary Labels: single column of binary labels representing positive and negative returns on the day\n",
    "###  - Additional Notes:\n",
    "        - Note that a negative return is defined as class 0 and a postive return or no return is defined as class 1\n",
    "        - default (0,1)\n",
    "        - optimized (-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_binary_classification(df, encoding):\n",
    "    if(encoding == \"default\"):\n",
    "        binary_classification = df.apply(lambda x: 0 if x < 0 else 1)\n",
    "    elif(encoding == \"optimized\"): \n",
    "        binary_classification = df.apply(lambda x: -1 if x < 0 else 1)\n",
    "    else:\n",
    "        print(\"ERROR: Input not recognized\")\n",
    "    return binary_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split_data\n",
    "###  - Inputs:\n",
    "        - df (DataFrame): Processed DataFrame with features and labels to be split\n",
    "        - train_percentage (float): Percentage of data to be used as training data - assumes all other data is used as test\n",
    "        - features (str): toggles whether to use text only or text and numerical features (valid inputs: 'text', 'all')\n",
    "        - label (str): selects target label based on column name\n",
    "        - target_type (str): binary (bin) or continuous (cont)\n",
    "        - encoding (str): default or optimized (see convert_to_binary_classification)\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - data_dict (Dictionary): dictionary with 4 values corresponding to X_train, X_test, Y_train, Y_test (keys)\n",
    "###  - Additional Notes:\n",
    "        - None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, train_percentage, features, label, target_type, encoding):\n",
    "    #Determine Split Index Based on the Percentage Of Data to Be Used For Training\n",
    "    train_index = int(train_percentage*len(df.index)) + 1\n",
    "    \n",
    "    #Split Data\n",
    "    if(features == 'text'):\n",
    "        X_train = df.iloc[0:train_index,20:]\n",
    "        X_test = df.iloc[train_index:,20:]\n",
    "    elif(features == 'all'):\n",
    "        df1 = df.iloc[0:train_index:,np.r_[5:7,8]]\n",
    "        df2 = df.iloc[0:train_index,20:]\n",
    "        X_train = pd.concat([df1,df2],axis=1)\n",
    "        \n",
    "        df3 = df.iloc[train_index:,np.r_[5:7,8]]\n",
    "        df4 = df.iloc[train_index:,20:]\n",
    "        X_test = pd.concat([df3,df4],axis=1)\n",
    "    else:\n",
    "        raise ValueError('Invalid Parameter Input - features')\n",
    "        \n",
    "    if(target_type == 'bin' or target_type == 'cont'):\n",
    "        if(target_type == 'bin'):\n",
    "            Y = convert_to_binary_classification(df[label], encoding)\n",
    "        else:\n",
    "            Y = df[label]\n",
    "        Y_train = Y.iloc[0:train_index]\n",
    "        Y_test = Y.iloc[train_index:]\n",
    "    else:\n",
    "        raise ValueError('Invalid Parameter Input - target_type')\n",
    "    \n",
    "    #Return as a Dictionary\n",
    "    data_dict = dict()\n",
    "    \n",
    "    data_dict['X_train'] = X_train\n",
    "    data_dict['X_test'] = X_test\n",
    "    data_dict['Y_train'] = Y_train\n",
    "    data_dict['Y_test'] = Y_test\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create_metrics_table\n",
    "###  - Inputs:\n",
    "        - Y_train: training labels\n",
    "        - pred_train: predicted labels from training data\n",
    "        - Y_test: training labels\n",
    "        - pred_test: predicted labels from test data\n",
    "        - valid input types (series, numpy array, DF columns) - must be column vectors\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - table (DataFrame): table with REC, PREC, F1 amd ACC for training and test predictions\n",
    "###  - Additional Notes:\n",
    "        - None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_table(Y_train, pred_train, Y_test, pred_test):\n",
    "    #Calculate Metrics\n",
    "    REC_train = recall_score(Y_train,pred_train)\n",
    "    PREC_train = precision_score(Y_train, pred_train)\n",
    "    F1_train = f1_score(Y_train, pred_train)\n",
    "    ACC_train = accuracy_score(Y_train, pred_train)\n",
    "\n",
    "    REC_test = recall_score(Y_test,pred_test)\n",
    "    PREC_test = precision_score(Y_test,pred_test)\n",
    "    F1_test = f1_score(Y_test, pred_test)\n",
    "    ACC_test = accuracy_score(Y_test, pred_test)\n",
    "\n",
    "    #Create Table\n",
    "    rows = [[\"Training\", REC_train, PREC_train, F1_train, ACC_train], [\"Test\", REC_test, PREC_test, F1_test,ACC_test]]\n",
    "    table = pd.DataFrame(rows, columns = [\"Dataset\", \"Recall\", \"Precision\", \"F1 Score\", \"Accuracy\"])\n",
    "    table.set_index(\"Dataset\", inplace =True)\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot_confusion_matrix\n",
    "###  - Inputs:\n",
    "        - matrix (confusion matrix): confusion matrix generated from sklearn confusion_matrix\n",
    "        - data_set_name (str): name of dataset ex. 'training' or 'test' \n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - None - void type function\n",
    "        - Displays plot\n",
    "###  - Additional Notes:\n",
    "        - Wrapper for seaborn plot code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(matrix, data_set_name):\n",
    "    sns.heatmap(matrix, annot=True, fmt= \".3f\", xticklabels = LABELS, yticklabels = LABELS)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"Actual Label\")\n",
    "    plt.title(\"Confusion Matrix for \" + data_set_name + \" Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot_ROC_curve\n",
    "###  - Inputs:\n",
    "        - FPR: False Positive Rate - Generated from sklearn roc_curve\n",
    "        - TPR: True Positive Rate - Generated from sklearn roc_curve\n",
    "        - data_set_name (str): name of dataset ex. 'training' or 'test'\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - None - void type function\n",
    "        - Displays plot\n",
    "###  - Additional Notes:\n",
    "        - Wrapper for matplotlib plot code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC_curve(FPR, TPR, data_set_name):\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "    plt.plot(FPR, TPR)\n",
    "    plt.xlabel(\"False Positive\")\n",
    "    plt.ylabel(\"True Positive\")\n",
    "    plt.title(\"ROC Curve for \" + data_set_name + \" Data\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# action_signal_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Inputs:\n",
    "    - Y_train: training data\n",
    "    - Y_test: test data\n",
    "    - pred_train: prediction on training data\n",
    "    - pred_test: prediction on test data\n",
    "    - threshold: threshold to map action signals to\n",
    "\n",
    "## - Outputs\n",
    "    - action_signal_train: training data mapped to action signals\n",
    "    - action_signal_test: test data mapped to action signals\n",
    "    - action_signal_predict_train: prediction on training data mapped to action signals\n",
    "    - action_signal_predict_test: prediction on test data mapped to action signals\n",
    "\n",
    "## - Additional Notes\n",
    "    - For the action signals, a \"positive\" action signal is any value greater than the threshold, and is mapped to 1. A mapping of zero is if the value is less than the threshold, but greater than the negative of the threshold, and is a \"no action\" signal. A mapping of -1 is if the value is less than the negative of the threshold, and is a \"negative\" action signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_signal_mapping(Y_train, Y_test, pred_train, pred_test, threshold):\n",
    "\n",
    "\n",
    "    threshold = .001\n",
    "\n",
    "\n",
    "    action_signal_train = np.copy(Y_train)\n",
    "    action_signal_train[action_signal_train < -1*threshold] = -1\n",
    "    action_signal_train[action_signal_train > threshold] = 1\n",
    "    action_signal_train[np.abs(action_signal_train) <= threshold] = 0\n",
    "\n",
    "    action_signal_test = np.copy(Y_test)\n",
    "    action_signal_test[action_signal_test < -1*threshold] = -1\n",
    "    action_signal_test[action_signal_test > threshold] = 1\n",
    "    action_signal_test[np.abs(action_signal_test) <= threshold] = 0\n",
    "\n",
    "    action_signal_predict_train = np.copy(pred_train)\n",
    "    action_signal_predict_train[action_signal_predict_train < -1*threshold] = -1\n",
    "    action_signal_predict_train[action_signal_predict_train > threshold] = 1\n",
    "    action_signal_predict_train[np.abs(action_signal_predict_train) <= threshold] = 0\n",
    "\n",
    "    action_signal_predict_test = np.copy(pred_test)\n",
    "    action_signal_predict_test[action_signal_predict_test < -1*threshold] = -1\n",
    "    action_signal_predict_test[action_signal_predict_test > threshold] = 1\n",
    "    action_signal_predict_test[np.abs(action_signal_predict_test) <= threshold] = 0\n",
    "\n",
    "    return action_signal_train, action_signal_test, action_signal_predict_train, action_signal_predict_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metric_mapping_action_signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Inputs\n",
    "     - action_signal_train: training data mapped to action signals\n",
    "     - action_signal_test: test data mapped to action signals\n",
    "     - action_signal_predict_train: prediction of training data mappped to action signals\n",
    "     - action_signal_predict_test: prediciton of test data mapped to action signals\n",
    "\n",
    "## - Outputs\n",
    "     - action_signal_train_metrics: training data mapped to action signals with \"no action\" signals removed\n",
    "     - action_signal_test_metrics: test data mapped to action signals with \"no action\" signals removed\n",
    "     - action_signal_predict_train_metrics: prediction of training data mappped to action signals with \"no action\" signals removed\n",
    "     - action_signal_predict_test_metrics: prediciton of test data mapped to action signals with \"no action\" signals removed\n",
    "\n",
    "## Additional notes:\n",
    "    - for the measurement of different metrics such as accuracy, precision, recall etc., we drop the all cases where a \"no action\" signal is predicted and the truth was \"positive\" or \"negative\" action. We also drop the reverse scenario, where the truth was \"no action\", and the prediction was \"positive\" or \"negative\" action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_mapping_action_signals(action_signal_train, action_signal_test, action_signal_predict_train, action_signal_predict_test):\n",
    "    action_signal_train_metrics = np.empty((action_signal_train.shape))\n",
    "    action_signal_test_metrics = np.empty((action_signal_test.shape))\n",
    "    action_signal_predict_train_metrics = np.empty((action_signal_predict_train.shape))\n",
    "    action_signal_predict_test_metrics = np.empty((action_signal_predict_test.shape))\n",
    "\n",
    "    train_len = action_signal_train.shape[0]\n",
    "    test_len = action_signal_test.shape[0]\n",
    "\n",
    "    for i in range(0, train_len):\n",
    "        if (np.abs(action_signal_train[i] + action_signal_predict_train[i]) == 1) or (action_signal_train[i] == 0 and action_signal_predict_train[i] == 0):\n",
    "            action_signal_train_metrics[i] = 0\n",
    "            action_signal_predict_train_metrics[i] = 0\n",
    "        else:\n",
    "            action_signal_train_metrics[i] = action_signal_train[i]\n",
    "            action_signal_predict_train_metrics[i] = action_signal_predict_train[i]\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(0, test_len):\n",
    "        if (np.abs(action_signal_test[j] + action_signal_predict_test[j]) == 1) or (action_signal_test[j] == 0 and action_signal_predict_test[j] == 0):\n",
    "            action_signal_test_metrics[j] = 0\n",
    "            action_signal_predict_test_metrics[j] = 0\n",
    "        else:\n",
    "            action_signal_test_metrics[j] = action_signal_test[j]\n",
    "            action_signal_predict_test_metrics[j] = action_signal_predict_test[j]\n",
    "            \n",
    "    \n",
    "        \n",
    "    action_signal_train_metrics =  action_signal_train_metrics[action_signal_train_metrics != 0]\n",
    "    action_signal_test_metrics =  action_signal_test_metrics[action_signal_test_metrics != 0]\n",
    "    action_signal_predict_train_metrics = action_signal_predict_train_metrics[action_signal_predict_train_metrics != 0]\n",
    "    action_signal_predict_test_metrics = action_signal_predict_test_metrics[action_signal_predict_test_metrics != 0]\n",
    "    \n",
    "\n",
    "    return action_signal_train_metrics, action_signal_test_metrics, action_signal_predict_train_metrics, action_signal_predict_test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_best_SVC\n",
    "###  - Inputs:\n",
    "        - model_type (str): toggles between selecting the best linear model or choosing from all types - valid inputs ('all', 'linear')\n",
    "        - alphas (list): list containing the distinct values of the regularization parameter to be chosen between\n",
    "        - folds (int): number of folds used for cross validation\n",
    "        - X_train (DataFrame): training feature data\n",
    "        - Y_train (DataFrame): training label data\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - best_model (estimator object): returns estimator selected based on the given parameters\n",
    "###  - Additional Notes:\n",
    "        - prints the parameters selected to generate the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_SVC(model_type, alphas, folds, X_train, Y_train, verbose):\n",
    "    if(model_type == 'linear'):\n",
    "        lin_svc = LinearSVC(dual=False ,max_iter=500000)\n",
    "        params = {'penalty':['l1','l2'],'C':alphas}\n",
    "        clf = GridSearchCV(estimator=lin_svc, param_grid=params, cv=folds)\n",
    "        clf.fit(X_train,Y_train)\n",
    "        \n",
    "        if(verbose == 1):\n",
    "            print(\"Best Parameters: \")\n",
    "            print(clf.best_params_)\n",
    "        \n",
    "        best_model = clf.best_estimator_\n",
    "    elif(model_type == 'all'):\n",
    "        svc = SVC(max_iter=500000)\n",
    "        params = {'C':alphas, 'kernel':['linear','poly','sigmoid','rbf']}\n",
    "        clf = GridSearchCV(estimator=svc, param_grid=params, cv=folds)\n",
    "        clf.fit(X_train,Y_train)\n",
    "        \n",
    "        if(verbose == 1):\n",
    "            print(\"Best Parameters: \")\n",
    "            print(clf.best_params_)\n",
    "        \n",
    "        best_model = clf.best_estimator_\n",
    "    else:\n",
    "        raise ValueError('Invalid Parameter Input - model_type')\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_best_knn_regressor\n",
    "###  - Inputs:\n",
    "        - lower_bound_k (int): lower bound of k values being selected from (inclusive)\n",
    "        - upper_bound_k (int): upper bound of k values being selected from (inclusive)\n",
    "        - step_size (int): step size between values of k in specified range [lower_bound_k, upper_bound_k]\n",
    "        - folds (int): number of folds used for cross validation\n",
    "        - X_train (DataFrame): training feature data\n",
    "        - Y_train (DataFrame): training label data\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - best_model (estimator object): returns estimator selected based on the given parameters\n",
    "###  - Additional Notes:\n",
    "        - prints the parameters selected to generate the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_knn_regressor(lower_bound_k, upper_bound_k, step_size, folds, X_train, Y_train):\n",
    "    #Create List of Possible K Values\n",
    "    k_vals = list()\n",
    "    while(lower_bound_k <= upper_bound_k):\n",
    "        k_vals.append(lower_bound_k)\n",
    "        lower_bound_k+=step_size\n",
    "    \n",
    "    #Define Parameters to Cross-Validate\n",
    "    params = {'n_neighbors':k_vals,'weights':['uniform','distance'],'p':[1,2,3]}\n",
    "    \n",
    "    #Select Best Parameters\n",
    "    knn = KNeighborsRegressor()\n",
    "\n",
    "    knn_reg = GridSearchCV(estimator=knn, param_grid=params, cv=folds)\n",
    "    knn_reg.fit(X_train,Y_train)\n",
    "        \n",
    "    print(\"Best Parameters: \")\n",
    "    print(knn_reg.best_params_)\n",
    "        \n",
    "    best_model = knn_reg.best_estimator_\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_best_SVR\n",
    "###  - Inputs:\n",
    "        - model_type (str): toggles between selecting the best linear model or choosing from all types - valid inputs ('all', 'linear')\n",
    "        - alphas (list): list containing the distinct values of the regularization parameter to be chosen between\n",
    "        - degrees (list) - different degrees of the polynomial kernel function to be chosen from if 'poly' is selected. Ignored by all other kernels \n",
    "        - folds (int): number of folds used for cross validation\n",
    "        - X_train (DataFrame): training feature data\n",
    "        - Y_train (DataFrame): training label data\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - best_model (estimator object): returns estimator selected based on the given parameters\n",
    "###  - Additional Notes:\n",
    "        - prints the parameters selected to generate the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_SVR(model_type, alphas, degrees, folds, X_train, Y_train):\n",
    "    if(model_type == 'linear'):\n",
    "        lin_svr = LinearSVR(dual = False, max_iter=500000)\n",
    "        params = {'loss':['squared_epsilon_insensitive'],'C':alphas, 'fit_intercept': [True, False]}\n",
    "        clf = GridSearchCV(estimator=lin_svr, param_grid=params, cv=folds)\n",
    "        clf.fit(X_train,Y_train)\n",
    "        \n",
    "        print(\"Best Parameters: \")\n",
    "        print(clf.best_params_)\n",
    "        \n",
    "        best_model = clf.best_estimator_\n",
    "    elif(model_type == 'all'):\n",
    "        svr = SVR(max_iter=500000)\n",
    "        params = {'C':alphas, 'kernel':['linear','poly','sigmoid','rbf'], 'degree': degrees }   #'gamma': ['scale','auto']\n",
    "        clf = GridSearchCV(estimator=svr, param_grid=params, cv=folds)\n",
    "        clf.fit(X_train,Y_train)\n",
    "        \n",
    "        print(\"Best Parameters: \")\n",
    "        print(clf.best_params_)\n",
    "        print(\"Ignore degree value if kernel is not 'poly' \")\n",
    "        best_model = clf.best_estimator_\n",
    "    else:\n",
    "        raise ValueError('Invalid Parameter Input - model_type')\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_neural_network_cf\n",
    "###  - Inputs:\n",
    "        - features (int): number of features - default none (will throw error if int not provided)\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - model (keras estimator): compiled estimator specified by the function\n",
    "###  - Additional Notes:\n",
    "        - Neural Network strucutre needs to be tuned by hand - function does not handle this\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neural_network_cf(features=None):\n",
    "    #Build Model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Input Layer\n",
    "    model.add(Dense(64, input_dim=features, activation='relu'))\n",
    "    \n",
    "    #Hidden Layers\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    \n",
    "    #Output Layers\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "              \n",
    "    #Compile Model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "              \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_neural_network_reg\n",
    "###  - Inputs:\n",
    "        - features (int): number of features - default none (will throw error if int not provided)\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - model (keras estimator): compiled estimator specified by the function\n",
    "###  - Additional Notes:\n",
    "        - Neural Network strucutre needs to be tuned by hand - function does not handle this\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neural_network_reg(features=None):\n",
    "    #Build Model\n",
    "    model = Sequential();\n",
    "\n",
    "    #Input Layer\n",
    "    model.add(Dense(64, input_dim=features, activation='relu'))\n",
    "\n",
    "    #Hidden Layers\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "\n",
    "    #Output Layer\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    #Compile Model\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # all_models_cf\n",
    " ###  - Inputs:\n",
    "        - file_name (str): name of file - ex. 'FB.txt\"\n",
    "        - csv_name (str): name of csv - ex. 'FB.csv'\n",
    "        - features (int): number of text features to include\n",
    "        - verbose (int): 0 to hide messages, 1 to show messages\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - result (list): list containing the name of the dataset, best model type, corresponding accuracy, and the fitted model\n",
    "###  - Additional Notes:\n",
    "        - none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_models_cf(file_name, csv_name, features, verbose):\n",
    "    all_features = features+3\n",
    "    name = file_name[:-4]\n",
    "\n",
    "    company = stock_twits_text_parser(file_name)\n",
    "    company_metrics = stock_twits_metrics(company, 3, file_name)\n",
    "    company_return = return_over_period_T(csv_name,3)\n",
    "    agg_data = feature_selector(company_metrics, company_return, 'close_return', 25, features, 'default')\n",
    "\n",
    "    if(features > agg_data.iloc[:,20:].shape[1]):\n",
    "        all_features =agg_data.iloc[:,20:].shape[1] + 3\n",
    "        \n",
    "    company_data = split_data(agg_data, 0.7, 'all', 'close_return', 'bin', \"default\")\n",
    "    \n",
    "    #Split\n",
    "    X_train = company_data['X_train']\n",
    "    Y_train = company_data['Y_train']\n",
    "    X_test = company_data['X_test']\n",
    "    Y_test = company_data['Y_test']\n",
    "\n",
    "    reg_vals = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4, 1e5]\n",
    "\n",
    "    clf_lin_svc =  get_best_SVC('linear', reg_vals, 5, X_train, Y_train,0)\n",
    "\n",
    "    model = KerasClassifier(build_fn=build_neural_network_cf, features=all_features, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "    best_accuracy = clf_lin_svc.score(X_test,Y_test)\n",
    "    best_classifier = 0\n",
    "\n",
    "    l1 = Pipeline([('clf_l1',LogisticRegressionCV(Cs=[1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4, 1e5], cv = 5, penalty = \"l1\", solver = 'liblinear'))])\n",
    "    l2 = Pipeline([('clf_l2',RidgeClassifierCV(alphas=[1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4, 1e5], cv = 5))])\n",
    "    rf = Pipeline([('clf_rf',RandomForestClassifier())])\n",
    "    et = Pipeline([('clf_et',ExtraTreesClassifier())])\n",
    "    nn = Pipeline([('clf_nn',model)])\n",
    "    \n",
    "    pipe_dict = {0:'SVC',1:'L1',2:\"L2\",3:\"Random Forest\",4:\"Extra Trees\",5:\"Neural Network\"}\n",
    "    pipelines =[l1,l2,rf,et,nn]\n",
    "\n",
    "    for pipe in pipelines:\n",
    "        pipe.fit(X_train, Y_train)\n",
    "\n",
    "    if(verbose == 1):\n",
    "        print(\"SVC Test Accuracy: \" + str(best_accuracy))\n",
    "        for i, model in enumerate(pipelines):\n",
    "            print(\"{} Test Accuracy: {}\".format(pipe_dict[i+1],model.score(X_test,Y_test)))\n",
    "            \n",
    "    for i, model in enumerate(pipelines):\n",
    "        if(model.score(X_test, Y_test)>best_accuracy):\n",
    "            best_accuracy = model.score(X_test,Y_test)\n",
    "            best_classifier = i+1\n",
    "    \n",
    "    if best_classifier == 0: # return SVC as best model\n",
    "        best_model = clf_lin_svc\n",
    "    else:\n",
    "        best_model = pipelines[best_classifier - 1] # return best model from pipelines\n",
    "    \n",
    "    print(\"Best Model: {}\".format(pipe_dict[best_classifier]) )\n",
    "    result = [name, pipe_dict[best_classifier], best_accuracy, best_model]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all_datasets_cf\n",
    "###  - Inputs:\n",
    "        - companies (list) - list of company abbreviations eg. FB, AXP, CAT\n",
    "        - n_features (int) - number of text features to include\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - table (DataFrame) - table of results containing the name of each stock, best estimator and the corresponding accuracy\n",
    "###  - Additional Notes:\\n\",\n",
    "        - none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_datasets_cf(companies, n_features):\n",
    "    company_performance = list()\n",
    "    for company in companies:\n",
    "        file_name = company + '.txt'\n",
    "        csv = company + '.csv'\n",
    "        features = n_features\n",
    "        result = all_models_cf(file_name, csv, features, 0)\n",
    "        company_performance.append(result)\n",
    "\n",
    "    #Create Table\n",
    "    rows = company_performance\n",
    "    table = pd.DataFrame(rows, columns = [\"Dataset\", \"Best Model\", \"Test Score\"])\n",
    "    table.set_index(\"Dataset\", inplace =True)\n",
    "                    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trader_single_stock\n",
    "###  - Inputs:\n",
    "        - predictions (numpy array): array of label predictions\n",
    "        - period (int): time in days of when we are predicting the return for\n",
    "        - metric (str): the label of the price being compared ex. 'close'\n",
    "        - numerical_data (DataFrame): use output of return_over_period_T function\n",
    "        - capital (int): initial amount of money for simulation\n",
    "        - index (int) start index of test set\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - capital (int): amount of money we have at the end of the testing period\n",
    "###  - Additional Notes:\n",
    "        - to make the function more modular we can use len(X_test) for index inputs until there is a better solution for slicing (would be nice to eliminate this parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trader_single_stock(predictions, period, metric, numerical_data, capital, index):\n",
    "    target = metric.lower() + '_return'\n",
    "    \n",
    "    r = numerical_data[target]\n",
    "    price = numerical_data[metric.upper()]\n",
    "    \n",
    "    labels = r.iloc[index:].values\n",
    "    price_labels = price.iloc[index:].values\n",
    "    \n",
    "    action = deque()\n",
    "    investment = deque()\n",
    "    \n",
    "    for i in range (len(predictions)):\n",
    "        #assume we always invest 1/3 of our available cash\n",
    "        cash = capital/3\n",
    "        \n",
    "        #too poor to buy stock :(\n",
    "        if(cash < price[i]):\n",
    "            continue\n",
    "\n",
    "        investment.append(cash)\n",
    "        capital-=cash\n",
    "        \n",
    "        if(i < period):\n",
    "            action.append(predictions[i])\n",
    "        else:\n",
    "            #make new prediction for t+T\n",
    "            action.append(predictions[i])\n",
    "        \n",
    "            #calculate return based on prediction\n",
    "            prediction = action.popleft()\n",
    "            past_investment = investment.popleft()\n",
    "            \n",
    "            if(prediction == 1):\n",
    "                #calculate long return\n",
    "                delta = labels[i-3]\n",
    "            elif(prediction == -1):\n",
    "                #calculate short return\n",
    "                delta = -labels[i-3]\n",
    "            else:\n",
    "                #did not invest T days ago\n",
    "                delta = 0\n",
    "            #calculate profit and update capital value\n",
    "            shares = past_investment/price_labels[i-3]\n",
    "            profit = (delta+1)*past_investment - 0.0075*shares\n",
    "            capital+=profit\n",
    "        #print(capital)\n",
    "        \n",
    "    for i in range(period):\n",
    "        capital+=investment.popleft()\n",
    "        \n",
    "    return capital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trade_portfolio\n",
    "\n",
    "###  - Inputs:\n",
    "        - stock_portfolio: list of stock ticker names that will be traded\n",
    "        - algo_of_choice: set to default as Linear SVM for now, will be integrated for any algo in the future. if set to 'optimal', best algorithm for each stock is used using all_models_cf function\n",
    "        - capital: amount of capital to invest, will be distributed evenly amongst the stocks\n",
    "        - metric (str): the label of the price being compared ex. 'close'\n",
    "        - num_features: number of chi squared features to use\n",
    "        - period: period of T day return\n",
    "###  - Returns Dataframe with Following Value Types:\n",
    "        - all_company_capital (int): amount of money we have at the end of the testing period for each stock in our portfolio\n",
    "###  - Additional Notes:\n",
    "        - will be integrated to run any algo in future updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trade_portfolio(stock_portfolio, algo_of_choice , capital, metric, num_features, period ):\n",
    "    \n",
    "    num_stocks = len(stock_portfolio)\n",
    "    # all_company_returns = {}\n",
    "    # all_company_preds = {}                   \n",
    "    all_company_capital = {}\n",
    "    \n",
    "    for stock in stock_portfolio:\n",
    "\n",
    "        file_name = stock + '.json'\n",
    "        csv_name = stock + '.csv'\n",
    "        \n",
    "        data =  return_cleaned_train_test_labels_split(stock, encoding = 'default')\n",
    "        X_train = data[0]\n",
    "        Y_train = data[1]\n",
    "        X_test = data[2]\n",
    "        Y_test = data[3]\n",
    "        company_return = data[4]\n",
    "        #Define Values for Regularization Parameter\n",
    "        reg_vals = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4, 1e5]\n",
    "        \n",
    "        \n",
    "        if algo_of_choice  == 'Linear SVM':\n",
    "            #Find Best Model\n",
    "            clf_lin_svc =  get_best_SVC('linear', reg_vals, 5, X_train, Y_train)\n",
    "\n",
    "            #Predict\n",
    "            pred_test = clf_lin_svc.predict(X_test)\n",
    "        \n",
    "        if algo_of_choice == 'optimal':\n",
    "            optimal_model = all_models_cf(file_name, csv_name, features = num_features, verbose = 1)\n",
    "            model = optimal_model[3]\n",
    "            pred_test = model.predict(X_test)\n",
    "            #print(pred_test)\n",
    "        \n",
    "        \n",
    "        #all_company_returns[stock] = company_return\n",
    "        #all_company_preds[stock] = pred_test\n",
    "        all_company_capital[stock] = trader_single_stock(pred_test, period, metric, company_return, capital/num_stocks, len(X_train))\n",
    "    \n",
    "    return all_company_capital\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# return_cleaned_train_test_labels_split\n",
    "\n",
    "### - Inputs:\n",
    "        - stock: ticker name of the stock\n",
    "        - encoding: default or optimized (see convert_to_binary_classification)\n",
    "### - Returns:\n",
    "        - data: array with format [X_train, Y_train, X_test, Y_test, company_return]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_cleaned_train_test_labels_split(stock, encoding):\n",
    "        \n",
    "    file_name = stock + '.json'\n",
    "    csv_name = stock + '.csv'\n",
    "    #Features\n",
    "    company = stock_twits_text_parser(file_name)\n",
    "    company_metrics = stock_twits_metrics(company, period, file_name)\n",
    "\n",
    "    #Labels\n",
    "    company_return = return_over_period_T(csv_name,period)\n",
    "\n",
    "    #Cleaned Data\n",
    "    agg_data = feature_selector(company_metrics, company_return, 'close_return', 25, num_features, encoding)\n",
    "    company_data = split_data(agg_data, 0.7, 'all', 'close_return', 'bin', encoding)\n",
    "\n",
    "    #Split\n",
    "    X_train = company_data['X_train']\n",
    "    Y_train = company_data['Y_train']\n",
    "    X_test = company_data['X_test']\n",
    "    Y_test = company_data['Y_test']\n",
    "\n",
    "    data = [X_train, Y_train, X_test, Y_test, company_return]\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfile_name = 'FB.json'\\ncsv_name = 'FB.csv'\\n\\ncompany = stock_twits_text_parser(file_name)\\ncompany_metrics = stock_twits_metrics(company, 3, file_name)\\ncompany_return = return_over_period_T(csv_name,3)\\nagg_data = feature_selector(company_metrics, company_return, 'close_return', 25, 500)\\n\\n\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "file_name = 'FB.json'\n",
    "csv_name = 'FB.csv'\n",
    "\n",
    "company = stock_twits_text_parser(file_name)\n",
    "company_metrics = stock_twits_metrics(company, 3, file_name)\n",
    "company_return = return_over_period_T(csv_name,3)\n",
    "agg_data = feature_selector(company_metrics, company_return, 'close_return', 25, 500)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#company_data = split_data(agg_data, 0.7, 'all', 'close_return', 'bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#company_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
